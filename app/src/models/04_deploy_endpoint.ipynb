{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Imports and Setup\n",
    "Sets up the environment and defines the path to custom modules."
   ],
   "id": "5564109c5d0eea50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.serverless import ServerlessInferenceConfig"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Session Configuration\n",
    "Sagemaker session configuration"
   ],
   "id": "36ee9941d944dbfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# --- 1. Infrastructure Connection (SSM) ---\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Project Configuration (Must match your terraform.tfvars)\n",
    "project_name = \"cbis-ddsm\"\n",
    "env = \"dev\"\n",
    "\n",
    "\n",
    "# Function to read Parameter Store\n",
    "def get_ssm_parameter(name):\n",
    "    try:\n",
    "        ssm = boto3.client('ssm', region_name=region)\n",
    "        response = ssm.get_parameter(Name=name)\n",
    "        return response['Parameter']['Value']\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error reading parameter {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"ðŸ”„ Loading infrastructure configuration via SSM...\")\n",
    "\n",
    "# Load Dynamic Variables\n",
    "bucket_ssm = get_ssm_parameter(f\"/{project_name}/{env}/s3_bucket_name\")\n",
    "role_ssm = get_ssm_parameter(f\"/{project_name}/{env}/sagemaker_role_arn\")\n",
    "\n",
    "# --- 2. Session Initialization ---\n",
    "if bucket_ssm and role_ssm:\n",
    "    bucket = bucket_ssm\n",
    "    role = role_ssm\n",
    "\n",
    "    # IMPORTANT: Initialize Session with the Terraform bucket\n",
    "    # This forces SageMaker to use your specific bucket for all artifacts\n",
    "    sess = sagemaker.Session(default_bucket=bucket)\n",
    "\n",
    "    print(\"âœ… MLOps Mode Activated: Using Terraform resources.\")\n",
    "else:\n",
    "    # Fallback to standard mode (Sandbox)\n",
    "    sess = sagemaker.Session()\n",
    "    bucket = sess.default_bucket()\n",
    "    role = sagemaker.get_execution_role()\n",
    "    print(\"âš ï¸ Sandbox Mode: Using default resources.\")\n",
    "\n",
    "prefix = \"cbis-ddsm-classification\"\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Role:   {role.split('/')[-1]}\")"
   ],
   "id": "a973857f8e64825f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Retrieve and Deploy the Best Model\n",
    "Auto-Discover Latest Tuning Job and Deploy Serveless Inference Endpoint"
   ],
   "id": "cd0ff4debdfaa769"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Auto-Discover Latest Tuning Job\n",
    "print(\"\\nðŸ”Ž Searching for the latest completed Hyperparameter Tuning Job...\")\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# List jobs sorted by creation time (descending)\n",
    "response = sm_client.list_hyperparameter_tuning_jobs(\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=1,\n",
    "    StatusEquals='Completed' # Only looks for successful jobs\n",
    ")\n",
    "\n",
    "if response['HyperparameterTuningJobSummaries']:\n",
    "    # Grab the name of the most recent one\n",
    "    tuning_job_name = response['HyperparameterTuningJobSummaries'][0]['HyperparameterTuningJobName']\n",
    "    print(f\"âœ… Latest Tuning Job found: {tuning_job_name}\")\n",
    "else:\n",
    "    # STOP EXECUTION HERE IF NOT FOUND\n",
    "    error_msg = \"No completed Tuning Job found! Please run notebook 02 first.\"\n",
    "    print(error_msg)\n",
    "    raise ValueError(error_msg) # This stops the notebook\n",
    "\n",
    "# Retrieve the Best Model\n",
    "print(f\"\\nAttaching to job '{tuning_job_name}' to find the best model...\")\n",
    "tuner = HyperparameterTuner.attach(tuning_job_name)\n",
    "\n",
    "# Find out which training job was the best\n",
    "best_training_job = tuner.best_training_job()\n",
    "print(f\"âœ… The winning model was training job: {best_training_job}\")\n",
    "\n",
    "# Serverless Endpoint Deployment\n",
    "print(\"\\nConfiguring Serverless Inference to save costs...\")\n",
    "\n",
    "# Memory Configuration: 4GB is safe for ResNet50\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=4096,\n",
    "    max_concurrency=5\n",
    ")\n",
    "\n",
    "print(\"Starting deployment... This might take a few minutes.\")\n",
    "\n",
    "# Deploy the best model\n",
    "predictor = tuner.deploy(\n",
    "    serverless_inference_config=serverless_config,\n",
    "    endpoint_name=\"cbis-ddsm-serverless-endpoint\"\n",
    ")\n",
    "\n",
    "print(f\"ðŸš€ Serverless Endpoint ready! Name: {predictor.endpoint_name}\")"
   ],
   "id": "b1a9ac793e7274c7",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Inference Test\n",
    "Invoke the Endpoint to test the Inference"
   ],
   "id": "5bb484c9525ba225"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# Prepare a Test Image ---\n",
    "s3 = boto3.client('s3')\n",
    "print(\"\\nSelecting a random image from validation set for testing...\")\n",
    "\n",
    "# Download validation manifest\n",
    "s3.download_file(bucket, f\"{prefix}/metadata/validation.lst\", \"val_temp.lst\")\n",
    "\n",
    "# Read the first line to get a valid image\n",
    "with open(\"val_temp.lst\", \"r\") as f:\n",
    "    line = f.readline()\n",
    "    parts = line.strip().split('\\t')\n",
    "    test_label = parts[1]\n",
    "    test_image_path = parts[2]\n",
    "\n",
    "# Remove temp file\n",
    "os.remove(\"val_temp.lst\")\n",
    "\n",
    "print(f\"Testing with image: {test_image_path}\")\n",
    "print(f\"Real Label (Expected): {test_label} (0=Benign, 1=Malignant)\")\n",
    "\n",
    "# Download the actual image file\n",
    "image_name = \"test_image.jpg\"\n",
    "s3.download_file(bucket, f\"{prefix}/images/{test_image_path}\", image_name)\n",
    "\n",
    "# Inference\n",
    "print(\"Sending image to Serverless Endpoint...\")\n",
    "\n",
    "with open(image_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "# Invoke the endpoint\n",
    "try:\n",
    "    response = predictor.predict(payload, initial_args={'ContentType': 'application/x-image'})\n",
    "\n",
    "    # Result Interpretation\n",
    "    result = json.loads(response)\n",
    "    prob_benign = result[0]\n",
    "    prob_malignant = result[1]\n",
    "\n",
    "    print(\"\\n--- AI Diagnostic Result ---\")\n",
    "    print(f\"Cancer Probability:   {prob_malignant:.4f} ({prob_malignant*100:.2f}%)\")\n",
    "    print(f\"Benign Probability:   {prob_benign:.4f} ({prob_benign*100:.2f}%)\")\n",
    "\n",
    "    if prob_malignant > 0.5:\n",
    "        print(\"Diagnosis: POSITIVE (Malignant)\")\n",
    "    else:\n",
    "        print(\"Diagnosis: NEGATIVE (Benign)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during inference: {str(e)}\")"
   ],
   "id": "bc893f65a5ca6f6a",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cleanup ( Optional )\n",
    "Used to delete endpoint after testing ( uncomment for default )"
   ],
   "id": "2dc14503227e5c69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cleanup ---\n",
    "# Uncomment below to delete endpoint after testing\n",
    "# print(\"\\nDeleting endpoint...\")\n",
    "# predictor.delete_endpoint()\n",
    "# print(\"Endpoint deleted.\")"
   ],
   "id": "44d950765e520b2c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
